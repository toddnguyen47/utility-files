= AWS Notes
Todd Nguyen
:toc:

== Summary

AWS Notes

== S3

* We need to translate from `web hosting` to API call
* API Gateway to split into CRUD lambda calls

== Module 2 - Building a Web App in AWS

* Audio files, host the applications
* S3 bucket to host our app
* Another S3 bucket to host our MP3 / audio files
* Lambda files in the BG doing functionality
* Store items in DynamoDB (limit of 400 Kb)
  ** Items too big to store in DynamoDB -> store in S3 buckets instead
* IAM to use authentication & authorization
* Use the API gateway to expose
  ** Cognito will be used for user authentication
* For S3 to be HTTPS, it needs to be behind a Load Balancer or a Cloud Distribution

== How to Access AWS programmatically

* Everything in AWS has to be a HTTPS signed requests
* 400 is client error (request was **NOT** ok)
* 500 is server error (request was ok)
* AWS management console is using the SDK in the background

**Why use AWS SDKs?**

* built-in resilience
  ** logic for retries / errors / timeouts
  ** retry is exponential backoff (wait for 1, 2, 4, 8, 16 seconds, etc)
* API:
  ** Low-level
    *** client connection
    *** one method per service operation
  ** High-level
    *** resource-level connection
    *** has one CLASS per conceptual resource
    *** defines service resource and individual resources

* Example S3 command

[source, shell]
----
aws s3 ls s3://mybucket --recursive

# <base_call> <service (command)> <subcommand> <target> <parameters>
----

* Synchronous / Blocking: Client makes a request and waits for command to complete
* Asynchronous:
  ** poll waiters to get status
  ** wait until table becomes ACTIVE
* Python3 error: `botocore.exceptions.ClientError`

## Module 3

.get caller identity
[source, shell]
----
aws sts get-caller-identity

# sts = secure <> <>
----

* Amazon CloudWatch allows you to monitor CPU, disk I/O, and network throughput
* Each AWS SDK implements automatic retry logic
* Can isolate AWS by locking version

## Module 4 - Getting Started with Permissions

* Look at IAM (AWS Identity and Access Management)
* IAM holds onto a collection of users
  ** Different set of app users instead of devs
* Preferable to assign permissions to a `User group` rather than a `user`
  ** Users become members of group
* **A group CANNOT contain another group!**
  ** Users can be part of multiple groups
* Policies and permissions -> Users, User Groups
* Policies and permissions -> Roles
  ** Temporary tokens can come into the principle of a role
* Resource-based policies vs. Identity-based policies
  ** Resource-based policies: resource can be s3 bucket;decryption ticket; etc
    *** `"Principal"`
    *** In the resource-based policies example, the `"Condition"` makes the deny policy not applied to certain IP addresses
  ** Identity-based policies:
    *** By annotating a `version`, you can use variables!
    *** No version = defaults to 2008. First line should always be `"Version"`
    *** `Principal` is ASSUMED on identity based policies

.Identity-based policy

[source, json]
----
{
  "Version": "2012-10-17",
  "Id": "s3policyId1",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Action": "",
      "Resource": "",
    }
  ]
}
----

* `arn` refer to objects, you'll need `notes/*` (the slash)
  ** bucket you don't need the slash
  ** `notes` isn't actually a folder in terms of S3. It's there for the conceptual aspect of a folder

**Permission Boundaries**

* Guard rail on an account / individual so they are limited so what they can do
* They are used to set MAXIMUM permissions; usually it's to deny all except for things stated in the `permission boundaries`

* IAM user accounts are NOT always required
  ** As long as we can get some identity, we can get the concept of `Roles`
  ** Roles also have permissions
  ** Roles and tokens are **TEMPORARY**; however, roles can be re-assumed
  ** Assume the role (API Call), then you'll get a new permission based on the role
* Roles gives us the ability to hand out permissions to anyone we want

**Roles: Example**

* Can request access to `UpdateApp` role -> temporary credentials are granted -> user update S3 bucket with role credentials
* Can assign `UpdateApp` role to a lambda function; lambda can then execute with that role permissions

* You can use different profiles, as long as the profile name matches in `.aws/config` and `.aws/credentials` (`credentials` is the PASSWORD file)

**Sign requests with credentials**

* We have to sign Signature Version 4 (SigV4)
* Why? So we can verify, protect, etc.
* SDK will sign for us (Yay!)
* Whenever you look at a role, look at the `Trust relationships` tab
  ** `"Principal"`` will show up!
  ** Allows AWS lambda to take on the particular role
* Temporary credentials do not need to be rotated or explicitly revoked; permanent (user groups / user accounts) need to be rotated or explicitly revoked

== Module 5 - S3

3 types of storages:

* Block Storage
* File Storage
* Object Storage
  ** S3 standard is the most expensive from a byte standpoint, but the cheapest in terms of accessibility
* S3 bucket has Object ACLs and Bucket ACLs, but they are never used nowadays as we prefer to use IAM permissions instead

**CLI**

* low-level commands: `s3api`
* high-level commands: `s3`

**Integrating with S3 using SDK**

* Create a S3 client (STUB) and use the client to make requests into the S3 service
* `s3client` is a "low level" client interface, `s3resource` is a "high level" resource interface
* `ETag` is the MD5 hash of the object
* Data is stored in S3 buckets as objects. Objects can be any kind of file
* An S3 bucket is NOT created globally; it is created within a region. It only has a globally unique name (dependent on a region)
* AWS SDKs define low-level APIs for Amazon S3, which are mapped to the underlining AWS REST API operations
* Enabling an S3 bucket for website hosting does NOT change its endpoint; it gives you a new endpoint
* All objects and buckets are PRIVATE by default
* Amazon S3 ACLs are NOT configured through IAM

## Module 6 - Processing Your Storage Operations

* Bucket Operations:
  ** Create
  ** List
  ** NO delete (?)
* 404 means bucket does not exist, so proceed with creating the bucket
* Bucket needs a region; its bucket name is unique within the AWS North America cloud / single namespace

**Working with Objects**

* Objects go into buckets
* Objects has a unique key within that bucket
* >= 5 GB: Consider uploading multi-parts
* Get a complete object or get an array of bytes


**S3 Select**

* Retrieveonly a subset of data from an object
* `InputSerliazation`

**Grant temporary access to objects**

* Request to a bucket
  ** Use a pre-signed URL
* Grants PUT or GET access
* Grant different types of permissions for the URLs
* Applies to ONE object
* Use parameter `--exclude` a S3 bucket
* Large scale processing of items

### S3 Batch Operations - Large-scale processing

* For large processes

**Host a static website**

* has default `index.html` and `error.html`
* static files only
* S3 cannot host HTTPS; need to expose S3 into a cloud front distribution

**Knowledge check**

* Recommended for multipart upload for objects larger than 100 MB in size
* Some services have APIs that require pagination (e.g. S3)
* With presigned URLs, you can share specific S3 objects with time-limited access
* Use S3 Select with SQL-like querying; cannot download OBJECTS, as S3 Select can only select some kind of table
* By default, Amazon S3 event notifications are NOT sent in response to any actions in Amazon S3
* A web server uses CORS to allow or deny the loading of resources stored within ANOTHER domain

## Best Practices

**Policies**

* Apply policies to groups
* Use the principle of LEAST privilege
* If we want to grant permissions, we need at least ONE allow, NO denies
* Bucket SHOULD be built out of a TEMPLATE
